\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here
\usepackage{listings}


% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero 29: Dennis Gankin, Daniel Mock}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table
\begin{description}
	\item [State representation] The states have only two diffrent attributes.
		\begin{description}
			\item[1.] The city the agent is currently in.
			\item[2.] Whether a task exists in this city and what the task's destination city would be.
		\end{description}
	\item [Possible actions] The agent can perform two different actions in each state. He can either move to another city or pickup a task if there is one in the current city. Both actions take the agent to a state with a new current city. Thus the actions do not allow the agent to stay in his current city.
	\item[Reward Table] The reward table stores all rewards for changing states. The actual reward for travelling from current city \(i\) to current city \(j\) is computed by \(R_{ij}=r_{ij}+c_{ij} \) with  \(r_{ij}\) being the expected task reward from city \(i\) to city \(j\) and \(c_{ij}\) being the travelling costs between the cities of the two states. If a task exists in the current state then \(r\) equals the task's expected reward otherwise \(r_{ij} =0\). 
	\item[Probability Transition Table] This table represents the probability to get from state \(s\) to state \(s'\). As the agent's goal is to maximize his reward the table stores the probablyility that there is a task available from the current city to the next state's city.  
\end{description}




\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented
A reactive agent is implemented. That means that the agent will compute his optimal actions in advance. 
The {\tt state} class represents the states with the variables {\tt currentCity}and {\tt taskCity}. {\tt taskCity} can equal {\tt null}. In this case there is no task available. Otherwise there is a task available from {\tt taskCity} to {\tt taskCity}. 
The method {\tt computeStrategy} returns a {\tt HashMap} that maps the optimal destination citiies to each possible state. The computation is done by value iteration. The vector \(V\) is also stored as a {\tt HashMap} that maps reward values to states. 
The method {\tt infiniteHorizzzon} computes the multiplication \(V*T\). \(T\) is represented by {\tt td.propability}. 
The reward table is stored in {\tt td.reward}. The optimal strategy is saved in the variable {\tt strategy} after computation.
The {\tt act()} method implements the agent's behaviour. The agent looks up his best action in the current state in the hashmap {\tt strategy}. If the city he will move to equals the available task's destination city then the agent picks up the task. Otherwise the agent moves without picking up the task. ..good enough value?


\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result
This experiment analyses the discount factor's influence on the computed strategy.  

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)
The experiment will test the discount factor's influence on the strategy's computation and on the average reward the agent gets after some amount of actions by varying the discount factor. In order to get reliable results the topology and the task distribution will stay fixed during the experiment. 

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results
A discount factor smaller than one makes the computation converge. The factor controls the importance of the future actions. That means the bigger the factor is the more future actions contribute to the sum. Therefore a higher discount factor causes more iterations when computing the optimal strategy with value iteration. With a small discount factor the future actions contribute much less to the sum and much less iterations are needed.  As is shown in \ref{table} the amount of iterations for the computation grows exponentially when increasing the discount factor. With a discount factor that is bigger or equal one the computation will not converge but it will result in an endless loop.

\begin{table*}
\centering
\begin{tabular}{l*{6}{c}r}
Discount factor             & 0.1 & 0.3 & 0.5 & 0.7  & 0.9 & 0.99& 0.999 \\
\hline
Amount of iterations        &12	& 21 & 33 &	57 &172& 1650&15051 \\
Average reward
\end{tabular}
\caption{discount factor's influence on the amount of iterations to compute the optimal strategy }
\label{table} 
\end{table*}





\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.



\subsubsection{Setting}
% you describe how you perform the experiment and you describe the dummy agent you created (you also need to specify the configuration used for the experiment)
This experiment compares the implemented reactive agent's actions and performance to a random agent and second dummy agent. The topology is fixed in  order to get comparable results. 
\begin{description}
\item[configuration]bla bla
\item[random agent]
The random agent choses his destination randomly. If there is a task at his current position going to the city he chose then the task is picked up and delivered otherwise the agent will move without the task.
\item [dummy agent]
The implemented dummy agent prioritizes the tasks. If a task is available in his current city he will always chose to pick up and deliver the task. Only when no task is available the dummy agent will chose a random city in the neighbourhood to go to. 


\end{description}



\subsubsection{Observations}
% elaborate on the observed results

\vdots

\subsection{Experiment n}
% other experiments you would like to present

\subsubsection{Setting}

\subsubsection{Observations}

\end{document}